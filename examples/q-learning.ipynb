{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-learning on grid-world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrd\n",
    "import jax\n",
    "from flax.struct import dataclass\n",
    "from nestedtuple import nestedtuple\n",
    "\n",
    "import jaxdp\n",
    "import jaxdp.mdp.sampler.mdp as sampler\n",
    "from jaxdp.mdp.mdp import MDP\n",
    "from jaxdp.learning.algorithms import q_learning, reducer\n",
    "import jaxdp.learning.reporter as reporter\n",
    "from jaxdp.typehints import QType\n",
    "from jax.typing import ArrayLike as KeyType\n",
    "\n",
    "\n",
    "# By default JAX set float types into float32. The line below enables\n",
    "# float64 data type.\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# Define the arguments\n",
    "@nestedtuple\n",
    "class Args:\n",
    "    seed: int = 42                     # Initial seeds\n",
    "    n_env: int = 8                     # Number of parallel environments for sampling\n",
    "\n",
    "    class policy_fn:\n",
    "        epsilon: float = 0.15          # Epsilon-greedy parameter\n",
    "\n",
    "    class update_fn:\n",
    "        alpha: float = 0.10            # Step size (a.k.a learning rate)\n",
    "\n",
    "    class train_loop:\n",
    "        gamma: float = 0.99            # Discount factor\n",
    "        n_steps: int = 1000            # Number of steps\n",
    "        eval_period: int = 50          # Evaluation period (in terms of <n_steps>)\n",
    "\n",
    "    class sampler_init:\n",
    "        queue_size: int = 50           # Queue size of the sampler for the metrics\n",
    "\n",
    "    class sampler_fn:\n",
    "        max_episode_len: int = 15      # Maximum length of an episode allowed by the sampler\n",
    "        rollout_len: int = 16          # Length of a rollout\n",
    "\n",
    "    class value_init:\n",
    "        minval: float = 0.0            # Minimum value of the uniform distribution\n",
    "        maxval: float = 1.0            # Maximum value of the uniform distribution\n",
    "\n",
    "    class mdp_init:\n",
    "        p_slip: float = 0.15           # Probability of slipping\n",
    "        board: Tuple[str] = (\"#####\",  # The board of the grid-world\n",
    "                             \"#  @#\",\n",
    "                             \"#  X#\",\n",
    "                             \"#P  #\",\n",
    "                             \"#####\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    \"\"\" Training state \"\"\"\n",
    "    key: KeyType\n",
    "    sampler: sampler.State\n",
    "    value: QType\n",
    "    mdp: MDP\n",
    "    report: reporter.asynchronous.ReportData\n",
    "\n",
    "\n",
    "@partial(jax.vmap,\n",
    "         in_axes=(State(0, 0, None, None, None), None),\n",
    "         out_axes=(0, State(0, 0, None, None, None)))\n",
    "def rollout_sample(state: State, arg: Args) -> Tuple[sampler.RolloutData, State]:\n",
    "    \"\"\" Collect n rollouts.\n",
    "\n",
    "    Args:\n",
    "        state (State): training state\n",
    "        arg (Args): train arguments\n",
    "\n",
    "    Returns:\n",
    "        Tuple[sampler.RolloutData, State]:\n",
    "            rollout data, updated training state\n",
    "    \"\"\"\n",
    "    key, sample_key = jrd.split(state.key, 2)\n",
    "    policy = jaxdp.e_greedy_policy.q(state.value, arg.policy_fn.epsilon)\n",
    "    rollout, sampler_state = sampler.rollout(\n",
    "        sample_key,\n",
    "        state.sampler,\n",
    "        policy,\n",
    "        state.mdp,\n",
    "        arg.sampler_fn.rollout_len,\n",
    "        arg.sampler_fn.max_episode_len)\n",
    "\n",
    "    return rollout, state.replace(key=key, sampler=sampler_state)\n",
    "\n",
    "\n",
    "@partial(jax.vmap, in_axes=(0, None, None), out_axes=State(0, 0, None, None, None))\n",
    "def init(sampler_key: KeyType, value_key: KeyType, arg: Args) -> State:\n",
    "    \"\"\" Initialize training state for n parallel rollout samplers\n",
    "\n",
    "    Args:\n",
    "        sampler_key (KeyType): sampler rng\n",
    "        value_key (KeyType): value initialization rng\n",
    "        arg (Args): train arguments\n",
    "\n",
    "    Returns:\n",
    "        State: initialized training state\n",
    "    \"\"\"\n",
    "    key, init_sampler_key = jrd.split(sampler_key, 2)\n",
    "    mdp = jaxdp.mdp.grid_world(**arg.mdp_init._asdict())\n",
    "    return State(\n",
    "        key,\n",
    "        sampler.init_sampler_state(init_sampler_key, mdp, arg.sampler_init.queue_size),\n",
    "        jrd.uniform(value_key, (mdp.action_size, mdp.state_size,),\n",
    "                    dtype=\"float\", **arg.value_init._asdict()),\n",
    "        mdp,\n",
    "        reporter.asynchronous.init_report(\n",
    "            arg.train_loop.n_steps,\n",
    "            arg.train_loop.eval_period)\n",
    "    )\n",
    "\n",
    "\n",
    "def update(rollout: sampler.RolloutData, state: State, arg: Args) -> QType:\n",
    "    \"\"\" Q-learning update with batch of samples\n",
    "\n",
    "    Args:\n",
    "        rollout (sampler.RolloutData): batch of rollouts\n",
    "        state (State): training state\n",
    "        arg (Args): train arguments\n",
    "\n",
    "    Returns:\n",
    "        QType: updated q values\n",
    "    \"\"\"\n",
    "\n",
    "    @partial(jax.vmap, in_axes=(0, None, None))\n",
    "    @partial(jax.vmap, in_axes=(0, None, None))\n",
    "    def value_step(sample: sampler.RolloutData, value: QType, gamma: float):\n",
    "        return q_learning.asynchronous.step(sample, value, gamma)\n",
    "\n",
    "    batch_next_value = value_step(rollout, state.value, arg.train_loop.gamma)\n",
    "    target_value = reducer.every_visit(rollout, batch_next_value)\n",
    "    next_value = q_learning.update(state.value, target_value, alpha=arg.update_fn.alpha)\n",
    "\n",
    "    return next_value\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"arg\"])\n",
    "def train(state: State, arg: Args) -> State:\n",
    "    \"\"\" Run training\n",
    "\n",
    "    Args:\n",
    "        state (State): training state\n",
    "        arg (Args): train arguments\n",
    "\n",
    "    Returns:\n",
    "        State: final training state\n",
    "    \"\"\"\n",
    "\n",
    "    def train_step(i: int, state: State):\n",
    "        rollout, state = rollout_sample(state, arg)\n",
    "        next_value = update(rollout, state, arg)\n",
    "\n",
    "        # Report the training metrics\n",
    "        report_data = (\n",
    "            state.sampler,\n",
    "            state.report,\n",
    "            state.mdp,\n",
    "            state.value,\n",
    "            next_value,\n",
    "            jnp.full_like(next_value, jnp.nan),\n",
    "            arg.train_loop.gamma,\n",
    "            i,\n",
    "            arg.train_loop.eval_period,\n",
    "        )\n",
    "\n",
    "        is_report = (i % arg.train_loop.eval_period) == (arg.train_loop.eval_period - 1)\n",
    "        report = jax.lax.cond(\n",
    "            is_report,\n",
    "            reporter.asynchronous.record,\n",
    "            lambda _, report, *__: report,\n",
    "            *report_data)\n",
    "\n",
    "        # Refresh reward queues after reporting\n",
    "        sampler_state = jax.lax.cond(\n",
    "            is_report,\n",
    "            sampler.refresh_queues,\n",
    "            lambda sampler_state: sampler_state,\n",
    "            state.sampler)\n",
    "\n",
    "        return state.replace(\n",
    "            sampler=sampler_state,\n",
    "            value=next_value,\n",
    "            report=report\n",
    "        )\n",
    "\n",
    "    return jax.lax.fori_loop(0, arg.train_loop.n_steps, train_step, state)\n",
    "\n",
    "\n",
    "arg = Args(sampler_init=Args.sampler_init(queue_size=100))\n",
    "key = jrd.PRNGKey(42)\n",
    "sampler_key, value_key = jrd.split(key, 2)\n",
    "\n",
    "state = init(jrd.split(sampler_key, arg.n_env), value_key, arg)\n",
    "final_state = train(state, arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from plot_util import make_figure\n",
    "\n",
    "\n",
    "# Make dataframe from the report\n",
    "report = final_state.report\n",
    "percentile = 25\n",
    "\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [[\"gridworld\"], [\"q-learning\"],\n",
    "     list(range(0, arg.train_loop.n_steps, arg.train_loop.eval_period))],\n",
    "    names=[\"ENV\", \"ALG\", \"STEP\"])\n",
    "columns = pd.MultiIndex.from_product(\n",
    "    [report.__dataclass_fields__, [\"low\", \"med\", \"high\"]],\n",
    "    names=[\"METRIC\", \"PERCENTILE\"])\n",
    "\n",
    "data = []\n",
    "for name in report.__dataclass_fields__:\n",
    "    values = getattr(report, name)\n",
    "    if values.ndim == 1:\n",
    "        values = values.reshape(1, -1)\n",
    "    percentiles = jnp.nanpercentile(\n",
    "        values, q=jnp.array([percentile, 50, 100 - percentile]), axis=0)\n",
    "    data.append(percentiles)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=jnp.stack(list(chain(*data)), axis=1), columns=columns, index=index)\n",
    "\n",
    "# Generate the figure\n",
    "make_figure(df.loc[\"gridworld\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxdp",
   "language": "python",
   "name": "jaxdp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
